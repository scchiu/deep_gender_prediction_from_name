{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import keras\n",
    "import sklearn\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf; from keras.backend.tensorflow_backend import set_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load baby name dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv('./dataset/babynames.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of female name is twice\n",
    "* 1 is male; 0 is female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    62587\n",
       "1    34723\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.gender.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling test data with balance classes\n",
    "* Instead of spliting train/test with stratified target, sampling test data with balance classes makes gender distribution much fit the real world. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with balance classes\n",
    "tt_split_ratio = 0.2\n",
    "target_col = db.gender\n",
    "min_n = int(target_col.value_counts().values[-1] * tt_split_ratio)\n",
    "test_index = []\n",
    "for cat in target_col.unique(): \n",
    "    test_index += list(db[target_col==cat].sample(min_n, random_state=0).index)\n",
    "train_db=db[~db.index.isin(test_index)].reset_index(drop=True)\n",
    "test_db=db[db.index.isin(test_index)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = '.'\n",
    "exp_name='gender_predict_by_name'\n",
    "par = {'embedding_dim': 20, \n",
    "         'target': 'gender', \n",
    "         'text_column': 'name', \n",
    "         'max_seq_len': max(train_db.name.apply(len)), \n",
    "         'model_dir': os.path.join(project_root, 'model', exp_name), \n",
    "         'result_dir': os.path.join(project_root, 'result/'), \n",
    "         'result_filename': exp_name, \n",
    "         'conv_kernel_size_1': 4, 'conv_filters_1': 2048, 'mp_filters_1': 1, \n",
    "         'conv_kernel_size_2': 2, 'conv_filters_2': 128, 'mp_filters_2': 1, \n",
    "         'fc_size_1': 1024, 'fc_size_2': 512, \n",
    "         'lr': 1e-4, 'decay': 0., 'dropout': 0.1, \n",
    "         'validation_ratio': 0.2, 'epochs': 200, 'batch_size': 32, 'patience': 2\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete ./model/gender_predict_by_name ...\n",
      "create folder ./model/gender_predict_by_name\n"
     ]
    }
   ],
   "source": [
    "# remove directory recursively\n",
    "import shutil\n",
    "if os.path.exists(par['model_dir']): \n",
    "    print(\"delete %s ...\"%(par['model_dir']))\n",
    "    shutil.rmtree(par['model_dir'])\n",
    "if os.path.exists(par['result_dir'] + par['result_filename']): \n",
    "    print(\"delete %s\" % (par['result_dir'] + par['result_filename']))\n",
    "    os.remove(par['result_dir'] + par['result_filename'])\n",
    "    \n",
    "# create model directory\n",
    "if not os.path.exists(par['model_dir']):\n",
    "    print(\"create folder %s\" % (par['model_dir']))\n",
    "    os.makedirs(par['model_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constraint GPU memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constraint gpu usage\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "* store what characters have been seen in training data\n",
    "* save feature engineering for later use and predict process\n",
    "* if these is numeric features, variable of normalized method will be stored, such as mean/std and min/max, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "tokenizer = Tokenizer(char_level=True)    \n",
    "tokenizer.fit_on_texts(train_db[par['text_column']].tolist())\n",
    "par['num_of_char'] = len(tokenizer.word_index)\n",
    "\n",
    "# save model and feature engineering file\n",
    "par['start_time']=datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S-%f\") \n",
    "par['model_file'] = 'model_' + par['start_time'] + '.hdf5'\n",
    "par['feature_engineering_file'] = par['model_file'] + '.fe'\n",
    "with open(os.path.join(par['model_dir'], par['feature_engineering_file']), 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / validation data split\n",
    "* make target balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_count = int(train_db['gender'].value_counts().sort_values()[0] * par['validation_ratio'])\n",
    "f_val_index = train_db[train_db['gender']==0].sample(val_count//2, random_state=0).index\n",
    "m_val_index = train_db[train_db['gender']==1].sample(val_count//2, random_state=0).index\n",
    "val_index = f_val_index.append(m_val_index)\n",
    "train_index = train_db.index.difference(val_index)\n",
    "val_db = train_db.loc[val_index].reset_index(drop=True)\n",
    "train_db = train_db.loc[train_index].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over-sampling\n",
    "* training data is unbalance\n",
    "* over-sampling data in minor class making class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# over-sampling\n",
    "maxlen = np.sort(train_db[par['target']].value_counts())[-1]\n",
    "for p in train_db[par['target']].unique():\n",
    "    plen = len(train_db[train_db[par['target']]==p])\n",
    "    train_db = train_db.append(train_db[train_db[par['target']]==p]\\\n",
    "                               .sample((maxlen-plen), replace=True)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transform for feeding into model\n",
    "* transform character sequence into index sequence\n",
    "* padding sequences with the same length\n",
    "* coding binary target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text = tokenizer.texts_to_sequences(train_db[par['text_column']].tolist())\n",
    "x_train_text = pad_sequences(x_train_text, maxlen=par['max_seq_len'])\n",
    "x_val_text = tokenizer.texts_to_sequences(val_db[par['text_column']].tolist())\n",
    "x_val_text = pad_sequences(x_val_text, maxlen=par['max_seq_len'])    \n",
    "x_test_text = tokenizer.texts_to_sequences(test_db[par['text_column']].tolist())\n",
    "x_test_text = pad_sequences(x_test_text, maxlen=par['max_seq_len'])\n",
    "\n",
    "y_train = keras.utils.to_categorical(train_db['gender'])\n",
    "y_val = keras.utils.to_categorical(val_db['gender'])\n",
    "y_test = keras.utils.to_categorical(test_db['gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(par):\n",
    "    # two 1-D CNN layers over embedding sequence\n",
    "    embedding_layer = keras.layers.Embedding(par['num_of_char'], par['embedding_dim'],\n",
    "                                             embeddings_initializer='uniform', \n",
    "                                             input_length=par['max_seq_len'], trainable=True)\n",
    "    sequence_input = keras.layers.Input(shape=(par['max_seq_len'],), dtype='int32')\n",
    "    embedding_sequences = embedding_layer(sequence_input)\n",
    "    x = keras.layers.Conv1D(par['conv_filters_1'], par['conv_kernel_size_1'], \n",
    "                            use_bias=False)(embedding_sequences)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.MaxPool1D(par['mp_filters_1'])(x)\n",
    "    x = keras.layers.Dropout(par['dropout'])(x)\n",
    "    x = keras.layers.Conv1D(par['conv_filters_2'], par['conv_kernel_size_2'], \n",
    "                            use_bias=False)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.MaxPool1D(par['mp_filters_2'])(x)\n",
    "    x = keras.layers.Dropout(par['dropout'])(x)\n",
    "    text_output = keras.layers.Flatten()(x)\n",
    "    \n",
    "    # two fully-connected layers\n",
    "    x = keras.layers.Dense(par['fc_size_1'], use_bias=False)(text_output)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.Dropout(par['dropout'])(x)\n",
    "    x = keras.layers.Dense(par['fc_size_2'], use_bias=False)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.Dropout(par['dropout'])(x)\n",
    "    preds = keras.layers.Dense(2, activation='softmax')(x)\n",
    "    model = keras.models.Model(inputs=sequence_input, outputs = preds)\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=par['lr'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=par['decay'])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc']) \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training process\n",
    "* setup callback list\n",
    "* train model\n",
    "* store result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 15, 20)            1040      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 12, 2048)          163840    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 12, 2048)          8192      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 12, 2048)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 12, 2048)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 2048)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 11, 128)           524288    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 11, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 11, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 11, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 11, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1408)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1441792   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524288    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 2,671,122\n",
      "Trainable params: 2,663,698\n",
      "Non-trainable params: 7,424\n",
      "_________________________________________________________________\n",
      "Train on 100158 samples, validate on 11128 samples\n",
      "Epoch 1/200\n",
      "100158/100158 [==============================] - 67s - loss: 0.3976 - acc: 0.8287 - val_loss: 0.3285 - val_acc: 0.8661.828 - ETA: 0s - loss: 0.3982 - ac\n",
      "Epoch 2/200\n",
      "100158/100158 [==============================] - 68s - loss: 0.3162 - acc: 0.8688 - val_loss: 0.3118 - val_acc: 0.8733\n",
      "Epoch 3/200\n",
      "100158/100158 [==============================] - 67s - loss: 0.2780 - acc: 0.8875 - val_loss: 0.2948 - val_acc: 0.8781\n",
      "Epoch 4/200\n",
      "100158/100158 [==============================] - 67s - loss: 0.2489 - acc: 0.9005 - val_loss: 0.2890 - val_acc: 0.8815\n",
      "Epoch 5/200\n",
      "100158/100158 [==============================] - 68s - loss: 0.2229 - acc: 0.9110 - val_loss: 0.2861 - val_acc: 0.8852\n",
      "Epoch 6/200\n",
      "100158/100158 [==============================] - 67s - loss: 0.2000 - acc: 0.9211 - val_loss: 0.3000 - val_acc: 0.8852\n",
      "Epoch 7/200\n",
      "100158/100158 [==============================] - 66s - loss: 0.1812 - acc: 0.9288 - val_loss: 0.2931 - val_acc: 0.8890\n",
      "Epoch 8/200\n",
      "100158/100158 [==============================] - 66s - loss: 0.1622 - acc: 0.9367 - val_loss: 0.3189 - val_acc: 0.8777\n",
      "Epoch 9/200\n",
      "100158/100158 [==============================] - 58s - loss: 0.1493 - acc: 0.9425 - val_loss: 0.3165 - val_acc: 0.8837\n",
      "Epoch 10/200\n",
      "100158/100158 [==============================] - 60s - loss: 0.1336 - acc: 0.9494 - val_loss: 0.3668 - val_acc: 0.8687\n",
      "13664/13888 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# training process\n",
    "start_time = time.time()\n",
    "model = model(par)\n",
    "\n",
    "# if no over-sampling, reassign class weight for unbalance target in training data\n",
    "class_weight = np.max(np.sum(y_train, axis=0)) / (np.sum(y_train, axis=0))\n",
    "callbacks_list = [\n",
    "    # setup early stop to avoid overfitting\n",
    "    keras.callbacks.EarlyStopping(monitor='val_acc', mode='max', patience=par['patience'], verbose=0), \n",
    "    # save accuracy-increased model after each epoch\n",
    "    keras.callbacks.ModelCheckpoint(os.path.join(par['model_dir'], par['model_file']), \n",
    "                                    monitor='val_acc', verbose=0, save_best_only=True, mode='max'), \n",
    "    # log for each epoch\n",
    "    keras.callbacks.CSVLogger(os.path.join(par['model_dir'], par['model_file'])+'.log'), \n",
    "    # log for tensorboard\n",
    "    keras.callbacks.TensorBoard(log_dir=os.path.join(par['model_dir'], par['model_file'])+'.tflog', \n",
    "                                histogram_freq=0, write_graph=True, write_images=True)\n",
    "]\n",
    "\n",
    "history = model.fit(x_train_text, y_train, validation_data=(x_val_text, y_val), \n",
    "                    class_weight=class_weight, epochs=par['epochs'], \n",
    "                    batch_size=par['batch_size'], shuffle=True, callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "par['train_time'] = (time.time() - start_time) / 60 # minutes\n",
    "min_i = np.argmin(np.array(history.history['val_loss']))\n",
    "par['run_epochs'] = min_i\n",
    "par['train_loss'] = history.history['loss'][min_i]\n",
    "par['train_acc'] = history.history['acc'][min_i]\n",
    "par['val_loss'] = history.history['val_loss'][min_i]\n",
    "par['val_acc'] = history.history['val_acc'][min_i]\n",
    "\n",
    "# testing process\n",
    "model.load_weights(os.path.join(par['model_dir'],par['model_file']))\n",
    "model.save(os.path.join(par['model_dir'],par['model_file']))\n",
    "\n",
    "test_result = model.evaluate(x_test_text, y_test)\n",
    "par['test_loss'] = test_result[0]\n",
    "par['test_acc'] = test_result[1]\n",
    "\n",
    "popularity_pred = (np.sum(y_train, axis=0) >= np.sort(np.sum(y_train, axis=0))[-1]).astype(int)\n",
    "par['bl_acc'] = np.sum(np.sum(y_test * popularity_pred, axis=1)) / y_test.shape[0] \n",
    "\n",
    "par['num_trainData'] = len(train_db)\n",
    "par['num_valData'] = len(val_db)\n",
    "par['num_testData'] = len(test_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test auccuray: 88.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'bl_acc': 1.0,\n",
       " 'conv_filters_1': 2048,\n",
       " 'conv_filters_2': 128,\n",
       " 'conv_kernel_size_1': 4,\n",
       " 'conv_kernel_size_2': 2,\n",
       " 'decay': 0.0,\n",
       " 'dropout': 0.1,\n",
       " 'embedding_dim': 20,\n",
       " 'epochs': 200,\n",
       " 'fc_size_1': 1024,\n",
       " 'fc_size_2': 512,\n",
       " 'feature_engineering_file': 'model_2018-11-26-16-31-13-359779.hdf5.fe',\n",
       " 'lr': 0.0001,\n",
       " 'max_seq_len': 15,\n",
       " 'model_dir': './model/gender_predict_by_name',\n",
       " 'model_file': 'model_2018-11-26-16-31-13-359779.hdf5',\n",
       " 'mp_filters_1': 1,\n",
       " 'mp_filters_2': 1,\n",
       " 'num_of_char': 52,\n",
       " 'num_testData': 13888,\n",
       " 'num_trainData': 100158,\n",
       " 'num_valData': 11128,\n",
       " 'patience': 2,\n",
       " 'result_dir': './result/',\n",
       " 'result_filename': 'gender_predict_by_name',\n",
       " 'run_epochs': 4,\n",
       " 'start_time': '2018-11-26-16-31-13-359779',\n",
       " 'target': 'gender',\n",
       " 'test_acc': 0.8883928571428571,\n",
       " 'test_loss': 0.30127137183525043,\n",
       " 'text_column': 'name',\n",
       " 'train_acc': 0.91100061904217888,\n",
       " 'train_loss': 0.22288919804679616,\n",
       " 'train_time': 11.015794666608175,\n",
       " 'val_acc': 0.88524442846872753,\n",
       " 'val_loss': 0.2861167724765864,\n",
       " 'validation_ratio': 0.2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune model via grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def output_result(par, result):\n",
    "#     OUTPUT_FILEPATH = par['result_dir'] + par['result_filename'] + '.csv'\n",
    "#     if os.path.isfile(OUTPUT_FILEPATH):\n",
    "#         print(\"result is appended to %s\" % (OUTPUT_FILEPATH))\n",
    "#         result.to_csv(OUTPUT_FILEPATH, encoding='utf-8', index=False, mode='a', header=False)    \n",
    "#     else:\n",
    "#         print(\"%s is created.\" % (OUTPUT_FILEPATH))\n",
    "#         result.to_csv(OUTPUT_FILEPATH, encoding='utf-8', index=False)  \n",
    "\n",
    "# import itertools\n",
    "# tp={'text_column':['name'],'embedding_dim':[40]\n",
    "#     ,'conv_kernel_size_1':[4],'conv_filters_1':[2048],'mp_filters_1':[1]\n",
    "#     ,'conv_kernel_size_2':[2],'conv_filters_2':[256,128,512],'mp_filters_2':[1,2,3]\n",
    "#     ,'dropout':[0.1],'fc_size_1':[512],'fc_size_2':[256]\n",
    "#     ,'lr':[1e-4],'decay':[0.,1e-5],'batch_size':[32],'epochs':[200],'patience':[6]}\n",
    "\n",
    "# tp_com=list(itertools.product(*[p for p in tp.values()]))\n",
    "\n",
    "# i=0\n",
    "# while i<len(tp_com):\n",
    "#     par=d_par.copy()    \n",
    "#     one_li = []\n",
    "#     print(\"\\n %s/%s %s\" % (i+1, len(tp_com), ''))\n",
    "    \n",
    "#     for k,j in zip(tp.keys(),tp_com[i]):\n",
    "#         par[k]=j\n",
    "        \n",
    "#     print(tp_com[i])\n",
    "#     result,model=process_all(par,train_db,test_db)\n",
    "#     if result is not None:\n",
    "#         one_li.append(result)\n",
    "#         one_result = pd.DataFrame(one_li)\n",
    "#         output_result(par, one_result)    \n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fin_keras222",
   "language": "python",
   "name": "fin_keras222"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
