{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import keras\n",
    "import sklearn\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf; from keras.backend.tensorflow_backend import set_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load baby name dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = pd.read_csv('./dataset/babynames.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of female name is twice\n",
    "* 1 is male; 0 is female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    62587\n",
       "1    34723\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.gender.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling test data with balance classes\n",
    "* Instead of spliting train/test with stratified target, sampling test data with balance classes makes gender distribution much fit the real world. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train/test split with balance classes\n",
    "tt_split_ratio = 0.2\n",
    "target_col = db.gender\n",
    "min_n = int(target_col.value_counts().values[-1] * tt_split_ratio)\n",
    "test_index = []\n",
    "for cat in target_col.unique(): \n",
    "    test_index += list(db[target_col==cat].sample(min_n, random_state=0).index)\n",
    "train_db=db[~db.index.isin(test_index)].reset_index(drop=True)\n",
    "test_db=db[db.index.isin(test_index)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_root = '.'\n",
    "exp_name='gender_predict_by_name'\n",
    "par = {'embedding_dim': 20, \n",
    "         'target': 'gender', \n",
    "         'text_column': 'name', \n",
    "         'max_seq_len': max(train_db.name.apply(len)), \n",
    "         'model_dir': os.path.join(project_root, 'model', exp_name), \n",
    "         'result_dir': os.path.join(project_root, 'result/'), \n",
    "         'result_filename': exp_name, \n",
    "         'conv_kernel_size_1': 4, 'conv_filters_1': 2048, 'mp_filters_1': 1, \n",
    "         'conv_kernel_size_2': 2, 'conv_filters_2': 128, 'mp_filters_2': 1, \n",
    "         'fc_size_1': 1024, 'fc_size_2': 512, \n",
    "         'lr': 1e-4, 'decay': 0., 'dropout': 0.1, \n",
    "         'validation_ratio': 0.2, 'epochs': 200, 'batch_size': 32, 'patience': 2\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete ./model/gender_predict_by_name ...\n",
      "create folder ./model/gender_predict_by_name\n"
     ]
    }
   ],
   "source": [
    "# remove directory recursively\n",
    "import shutil\n",
    "if os.path.exists(par['model_dir']): \n",
    "    print(\"delete %s ...\"%(par['model_dir']))\n",
    "    shutil.rmtree(par['model_dir'])\n",
    "if os.path.exists(par['result_dir'] + par['result_filename']): \n",
    "    print(\"delete %s\" % (par['result_dir'] + par['result_filename']))\n",
    "    os.remove(par['result_dir'] + par['result_filename'])\n",
    "    \n",
    "# create model directory\n",
    "if not os.path.exists(par['model_dir']):\n",
    "    print(\"create folder %s\" % (par['model_dir']))\n",
    "    os.makedirs(par['model_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constraint GPU memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # constraint gpu usage\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "* store what characters have been seen in training data\n",
    "* save feature engineering for later use and predict process\n",
    "* if these is numeric features, variable of normalized method will be stored, such as mean/std and min/max, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit\n",
    "tokenizer = Tokenizer(char_level=True)    \n",
    "tokenizer.fit_on_texts(train_db[par['text_column']].tolist())\n",
    "par['num_of_char'] = len(tokenizer.word_index)\n",
    "\n",
    "# save model and feature engineering file\n",
    "par['start_time']=datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S-%f\") \n",
    "par['model_file'] = 'model_' + par['start_time'] + '.hdf5'\n",
    "par['feature_engineering_file'] = par['model_file'] + '.fe'\n",
    "with open(os.path.join(par['model_dir'], par['feature_engineering_file']), 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / validation data split\n",
    "* make target balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_count = int(train_db['gender'].value_counts().sort_values()[0] * par['validation_ratio'])\n",
    "f_val_index = train_db[train_db['gender']==0].sample(val_count//2, random_state=0).index\n",
    "m_val_index = train_db[train_db['gender']==1].sample(val_count//2, random_state=0).index\n",
    "val_index = f_val_index.append(m_val_index)\n",
    "train_index = train_db.index.difference(val_index)\n",
    "val_db = train_db.loc[val_index].reset_index(drop=True)\n",
    "train_db = train_db.loc[train_index].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over-sampling\n",
    "* training data is unbalance\n",
    "* over-sampling data in minor class making class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# over-sampling\n",
    "maxlen = np.sort(train_db[par['target']].value_counts())[-1]\n",
    "for p in train_db[par['target']].unique():\n",
    "    plen = len(train_db[train_db[par['target']]==p])\n",
    "    train_db = train_db.append(train_db[train_db[par['target']]==p]\\\n",
    "                               .sample((maxlen-plen), replace=True)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transform for feeding into model\n",
    "* transform character sequence into index sequence\n",
    "* padding sequences with the same length\n",
    "* coding binary target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_text = tokenizer.texts_to_sequences(train_db[par['text_column']].tolist())\n",
    "x_train_text = pad_sequences(x_train_text, maxlen=par['max_seq_len'])\n",
    "x_val_text = tokenizer.texts_to_sequences(val_db[par['text_column']].tolist())\n",
    "x_val_text = pad_sequences(x_val_text, maxlen=par['max_seq_len'])    \n",
    "x_test_text = tokenizer.texts_to_sequences(test_db[par['text_column']].tolist())\n",
    "x_test_text = pad_sequences(x_test_text, maxlen=par['max_seq_len'])\n",
    "\n",
    "y_train = keras.utils.to_categorical(train_db['gender'])\n",
    "y_val = keras.utils.to_categorical(val_db['gender'])\n",
    "y_test = keras.utils.to_categorical(test_db['gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(par):\n",
    "    # two 1-D CNN layers over embedding sequence\n",
    "    embedding_layer = keras.layers.Embedding(par['num_of_char'], par['embedding_dim'],\n",
    "                                             embeddings_initializer='uniform', \n",
    "                                             input_length=par['max_seq_len'], trainable=True)\n",
    "    sequence_input = keras.layers.Input(shape=(par['max_seq_len'],), dtype='int32')\n",
    "    embedding_sequences = embedding_layer(sequence_input)\n",
    "    x = keras.layers.Conv1D(par['conv_filters_1'], par['conv_kernel_size_1'], \n",
    "                            use_bias=False)(embedding_sequences)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.MaxPool1D(par['mp_filters_1'])(x)\n",
    "    x = keras.layers.Dropout(par['dropout'])(x)\n",
    "    x = keras.layers.Conv1D(par['conv_filters_2'], par['conv_kernel_size_2'], \n",
    "                            use_bias=False)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.MaxPool1D(par['mp_filters_2'])(x)\n",
    "    x = keras.layers.Dropout(par['dropout'])(x)\n",
    "    text_output = keras.layers.Flatten()(x)\n",
    "    \n",
    "    # two fully-connected layers\n",
    "    x = keras.layers.Dense(par['fc_size_1'], use_bias=False)(text_output)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.Dropout(par['dropout'])(x)\n",
    "    x = keras.layers.Dense(par['fc_size_2'], use_bias=False)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.Dropout(par['dropout'])(x)\n",
    "    preds = keras.layers.Dense(2, activation='softmax')(x)\n",
    "    model = keras.models.Model(inputs=sequence_input, outputs = preds)\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=par['lr'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=par['decay'])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc']) \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training process\n",
    "* setup callback list\n",
    "* train model\n",
    "* store result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 15, 20)            520       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 12, 2048)          163840    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 12, 2048)          8192      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 12, 2048)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 12, 2048)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 2048)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 11, 128)           524288    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 11, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 11, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 11, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 11, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1408)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1441792   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524288    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 2,670,602\n",
      "Trainable params: 2,663,178\n",
      "Non-trainable params: 7,424\n",
      "_________________________________________________________________\n",
      "Train on 100158 samples, validate on 11128 samples\n",
      "Epoch 1/200\n",
      "   192/100158 [..............................] - ETA: 28:02 - loss: 0.8329 - acc: 0.4948"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[9,11] = 26 is not in [0, 26)\n\t [[{{node embedding_1/embedding_lookup}} = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training/Adam/Assign_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_1/embeddings/read, _arg_input_1_0_0, training/Adam/gradients/embedding_1/embedding_lookup_grad/concat/axis)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7858c9759b68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0;31m#callbacks=callbacks_list,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                     verbose=1)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mpar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m60\u001b[0m \u001b[0;31m# minutes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[9,11] = 26 is not in [0, 26)\n\t [[{{node embedding_1/embedding_lookup}} = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training/Adam/Assign_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_1/embeddings/read, _arg_input_1_0_0, training/Adam/gradients/embedding_1/embedding_lookup_grad/concat/axis)]]"
     ]
    }
   ],
   "source": [
    "# training process\n",
    "start_time = time.time()\n",
    "model = model(par)\n",
    "\n",
    "# if no over-sampling, reassign class weight for unbalance target in training data\n",
    "class_weight = np.max(np.sum(y_train, axis=0)) / (np.sum(y_train, axis=0))\n",
    "callbacks_list = [\n",
    "    # setup early stop to avoid overfitting\n",
    "    keras.callbacks.EarlyStopping(monitor='val_acc', mode='max', patience=par['patience'], verbose=0), \n",
    "    # save accuracy-increased model after each epoch\n",
    "    keras.callbacks.ModelCheckpoint(os.path.join(par['model_dir'], par['model_file']), \n",
    "                                    monitor='val_acc', verbose=0, save_best_only=True, mode='max'), \n",
    "    # log for each epoch\n",
    "    keras.callbacks.CSVLogger(os.path.join(par['model_dir'], par['model_file'])+'.log'), \n",
    "    # log for tensorboard\n",
    "    keras.callbacks.TensorBoard(log_dir=os.path.join(par['model_dir'], par['model_file'])+'.tflog', \n",
    "                                histogram_freq=0, write_graph=True, write_images=True)\n",
    "]\n",
    "\n",
    "history = model.fit(x_train_text, y_train, validation_data=(x_val_text, y_val), \n",
    "                    class_weight=class_weight, epochs=par['epochs'], \n",
    "                    batch_size=par['batch_size'], shuffle=True, \n",
    "                    #callbacks=callbacks_list, \n",
    "                    verbose=1)\n",
    "\n",
    "par['train_time'] = (time.time() - start_time) / 60 # minutes\n",
    "min_i = np.argmin(np.array(history.history['val_loss']))\n",
    "par['run_epochs'] = min_i\n",
    "par['train_loss'] = history.history['loss'][min_i]\n",
    "par['train_acc'] = history.history['acc'][min_i]\n",
    "par['val_loss'] = history.history['val_loss'][min_i]\n",
    "par['val_acc'] = history.history['val_acc'][min_i]\n",
    "\n",
    "# testing process\n",
    "model.load_weights(os.path.join(par['model_dir'],par['model_file']))\n",
    "model.save(os.path.join(par['model_dir'],par['model_file']))\n",
    "\n",
    "test_result = model.evaluate(x_test_text, y_test)\n",
    "par['test_loss'] = test_result[0]\n",
    "par['test_acc'] = test_result[1]\n",
    "\n",
    "popularity_pred = (np.sum(y_train, axis=0) >= np.sort(np.sum(y_train, axis=0))[-1]).astype(int)\n",
    "par['bl_acc'] = np.sum(np.sum(y_test * popularity_pred, axis=1)) / y_test.shape[0] \n",
    "\n",
    "par['num_trainData'] = len(train_db)\n",
    "par['num_valData'] = len(val_db)\n",
    "par['num_testData'] = len(test_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test auccuray: 88.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune model via grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def output_result(par, result):\n",
    "#     OUTPUT_FILEPATH = par['result_dir'] + par['result_filename'] + '.csv'\n",
    "#     if os.path.isfile(OUTPUT_FILEPATH):\n",
    "#         print(\"result is appended to %s\" % (OUTPUT_FILEPATH))\n",
    "#         result.to_csv(OUTPUT_FILEPATH, encoding='utf-8', index=False, mode='a', header=False)    \n",
    "#     else:\n",
    "#         print(\"%s is created.\" % (OUTPUT_FILEPATH))\n",
    "#         result.to_csv(OUTPUT_FILEPATH, encoding='utf-8', index=False)  \n",
    "\n",
    "# import itertools\n",
    "# tp={'text_column':['name'],'embedding_dim':[40]\n",
    "#     ,'conv_kernel_size_1':[4],'conv_filters_1':[2048],'mp_filters_1':[1]\n",
    "#     ,'conv_kernel_size_2':[2],'conv_filters_2':[256,128,512],'mp_filters_2':[1,2,3]\n",
    "#     ,'dropout':[0.1],'fc_size_1':[512],'fc_size_2':[256]\n",
    "#     ,'lr':[1e-4],'decay':[0.,1e-5],'batch_size':[32],'epochs':[200],'patience':[6]}\n",
    "\n",
    "# tp_com=list(itertools.product(*[p for p in tp.values()]))\n",
    "\n",
    "# i=0\n",
    "# while i<len(tp_com):\n",
    "#     par=d_par.copy()    \n",
    "#     one_li = []\n",
    "#     print(\"\\n %s/%s %s\" % (i+1, len(tp_com), ''))\n",
    "    \n",
    "#     for k,j in zip(tp.keys(),tp_com[i]):\n",
    "#         par[k]=j\n",
    "        \n",
    "#     print(tp_com[i])\n",
    "#     result,model=process_all(par,train_db,test_db)\n",
    "#     if result is not None:\n",
    "#         one_li.append(result)\n",
    "#         one_result = pd.DataFrame(one_li)\n",
    "#         output_result(par, one_result)    \n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
